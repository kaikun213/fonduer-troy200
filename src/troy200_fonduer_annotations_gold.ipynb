{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Column/Row Heading with Data information from Tabby200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we use `Fonduer` to extract relations from the `Tabby200` dataset.  \n",
    "This code is a modified version of their original hardware [tutorial](https://github.com/HazyResearch/fonduer-tutorials/tree/master/hardware).  \n",
    "The `Fonduer` pipeline (as outlined in the [paper](https://arxiv.org/abs/1703.05028)), and the iterative KBC process:\n",
    "\n",
    "1. KBC Initialization\n",
    "2. Candidate Generation and Multimodal Featurization\n",
    "3. Probabilistic Relation Classification\n",
    "4. Error Analysis and Iterative KBC\n",
    "\n",
    "Additionally we assume that the dataset of spreadsheets has been preprocessed with cell annotations and table recognition as described in [paper1](https://ieeexplore.ieee.org/document/8970946/), [paper2](https://wwwdb.inf.tu-dresden.de/wp-content/uploads/demo_paper.pdf). The spreadsheets are converted to HTML/PDF format by libreoffice: `libreoffice --headless --calc --convert-to html --outdir html/ spreadsheet/*`. This results in an HTML format which is one large table. However, the cells are styled to indicate their classification:\n",
    "\n",
    "* \"Table\": 'bgcolor=#FFFFFE' background\n",
    "* \"Data\": 'color=#000001' font\n",
    "* \"Header\": 'color=#000002' font\n",
    "* \"MetaTitle\": 'color=#000003' font\n",
    "* \"Notes\": 'color=#000004' font "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we import the relevant libraries and connect to the local database.  \n",
    "Follow the README instructions to setup the connection to the postgres DB correctly.\n",
    "\n",
    "If the database has existing candidates with generated features, the will not be overriden.  \n",
    "To re-run the entire pipeline including initialization drop the database first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dropdb -h postgres -h postgres -h postgres -h postgres --if-exists troy200_col_row_data_gold\n",
    "! createdb -h postgres -h postgres -h postgres -h postgres troy200_col_row_data_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PARALLEL = 8 # 4  # assuming a quad-core machine\n",
    "ATTRIBUTE = \"troy200_col_row_data_gold\"\n",
    "\n",
    "DB_USERNAME = 'user'\n",
    "DB_PASSWORD = 'venron'\n",
    "conn_string = f'postgresql://{DB_USERNAME}:{DB_PASSWORD}@postgres:5432/{ATTRIBUTE}'\n",
    "    \n",
    "docs_path = 'data/gold/html/'\n",
    "pdf_path = 'data/gold/pdf/'\n",
    "gold_file = 'data/troy200_gold.csv'\n",
    "max_docs = 200 # 50 # 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "We first initialize a `Meta` object, which manages the connection to the database automatically, and enables us to save intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fonduer import Meta, init_logging\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "init_logging(log_dir=\"logs\")\n",
    "\n",
    "session = Meta.init(conn_string).Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fonduer.parser.preprocessors import HTMLDocPreprocessor\n",
    "from fonduer.parser.models import Document, Sentence\n",
    "from fonduer.parser import Parser\n",
    "\n",
    "has_documents = session.query(Document).count() > 0\n",
    "\n",
    "corpus_parser = Parser(session, structural=True, lingual=True, visual=True, pdf_path=pdf_path)\n",
    "\n",
    "if (not has_documents): \n",
    "    doc_preprocessor = HTMLDocPreprocessor(docs_path, max_docs=max_docs)\n",
    "    %time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)\n",
    "    \n",
    "print(f\"Documents: {session.query(Document).count()}\")\n",
    "print(f\"Sentences: {session.query(Sentence).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dividing the Corpus into Test and Train\n",
    "\n",
    "We'll split the documents 80/10/10 into train/dev/test splits. Note that here we do this in a non-random order to preserve the consistency and we reference the splits by 0/1/2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "all_docs = [train_docs, dev_docs, test_docs]\n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs][0:5])\n",
    "print(f\"Number of documents split: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Mention Extraction, Candidate Extraction Multimodal Featurization\n",
    "\n",
    "Given the unified data model from Phase 1, `Fonduer` extracts relation\n",
    "candidates based on user-provided **matchers** and **throttlers**. Then,\n",
    "`Fonduer` leverages the multimodality information captured in the unified data\n",
    "model to provide multimodal features for each candidate.\n",
    "\n",
    "## 2.1 Mention Extraction & Candidate Generation\n",
    "\n",
    "1. Define mention classes\n",
    "2. Use matcher functions to define the format of potential mentions\n",
    "3. Define Mentionspaces (Ngrams)\n",
    "4. Run Mention extraction (all possible ngrams in the document, API [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/candidates.html#fonduer.candidates.MentionExtractor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import mention_subclass\n",
    "from fonduer.candidates.matchers import RegexMatchSpan, LambdaFunctionMatcher, Intersect, Union\n",
    "from fonduer.utils.data_model_utils.structural import _get_node\n",
    "from fonduer.candidates import MentionNgrams, MentionSentences\n",
    "from fonduer.candidates import MentionExtractor \n",
    "from fonduer.candidates.models import Mention\n",
    "\n",
    "hasMentions = session.query(Mention).count() > 0\n",
    "\n",
    "# 1.) Mention subclasses\n",
    "Data = mention_subclass(\"Data\")\n",
    "Row = mention_subclass(\"Row\")\n",
    "Col = mention_subclass(\"Col\")\n",
    "\n",
    "def get_label_matcher(label):\n",
    "    def label_matcher(mention):\n",
    "        html_attrs = mention.sentence.html_attrs\n",
    "        parent_attrs = [f\"{k}={v}\" for k,v in _get_node(mention.sentence).getparent().attrib.items()]\n",
    "        \n",
    "        return {\n",
    "            \"Table\": 'bgcolor=#FFFFFE' in (html_attrs + parent_attrs) ,\n",
    "            \"Data\": 'color=#000001' in html_attrs,\n",
    "            \"Header\": 'color=#000002' in html_attrs,\n",
    "            \"MetaTitle\": 'color=#000003' in html_attrs,\n",
    "            \"Notes\": 'color=#000004' in html_attrs,\n",
    "        }[label]\n",
    "    return label_matcher\n",
    "\n",
    "if (not hasMentions):\n",
    "\n",
    "    # 2.) Matcher functions\n",
    "    # Regex: Match any numbers, including points, commas, percentage, minus or the format \"7 to 8\" or simply \"x\"\n",
    "    data_regex_matcher = RegexMatchSpan(rgx=r\"[0-9-,.%$#]+( to | )?[0-9-,.%$#]*|^x$\", longest_match_only=True)\n",
    "    data_label_matcher = LambdaFunctionMatcher(func=get_label_matcher(\"Data\"))\n",
    "    data_matcher = Intersect(data_regex_matcher, data_label_matcher)\n",
    "    # Regex-Matcher for only matching the longest string in all Headers\n",
    "    row_regex_matcher = RegexMatchSpan(rgx=r\"^.*$\", longest_match_only=True)\n",
    "    row_label_matcher = LambdaFunctionMatcher(func=get_label_matcher(\"Header\"))\n",
    "    row_matcher = Intersect(row_regex_matcher, row_label_matcher)\n",
    "    col_regex_matcher = RegexMatchSpan(rgx=r\"^.*$\", longest_match_only=True)\n",
    "    col_label_matcher = LambdaFunctionMatcher(func=get_label_matcher(\"Header\"))\n",
    "    col_matcher = Intersect(col_regex_matcher, col_label_matcher)\n",
    "\n",
    "    # 3.) Mention spaces (Ngrams)\n",
    "    data_ngrams = MentionSentences() # MentionNgrams(n_max=3)\n",
    "    row_ngrams = MentionSentences() # MentionNgrams(n_min=1, n_max=8)\n",
    "    col_ngrams = MentionSentences() # MentionNgrams(n_min=1, n_max=8)\n",
    "\n",
    "    # 4.) Mention extraction\n",
    "    mention_extractor = MentionExtractor(\n",
    "        session, [Data, Row, Col],  [data_ngrams, row_ngrams, col_ngrams], [data_matcher, row_matcher, col_matcher]\n",
    "    )\n",
    "    docs = session.query(Document).order_by(Document.name).all()\n",
    "    mention_extractor.apply(docs, parallelism=PARALLEL)\n",
    "\n",
    "    \n",
    "print(f\"Total Mentions: {session.query(Mention).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = session.query(Mention).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Candidate Extraction\n",
    "\n",
    "1. Define Candidate Class\n",
    "2. Define trottlers to reduce the number of possible candidates\n",
    "3. Extract candidates (View the API for the CandidateExtractor on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/candidates.html#fonduer.candidates.MentionExtractor).)\n",
    "\n",
    "In the last part we specified that these `Candidates` belong to the training set by specifying `split=0`; recall that we're referring to train/dev/test as splits 0/1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.data_model_utils import *\n",
    "from fonduer.utils.utils_table import is_row_aligned, is_col_aligned\n",
    "import re\n",
    "from fonduer.candidates import CandidateExtractor\n",
    "from fonduer.candidates.models import candidate_subclass\n",
    "from fonduer.utils.visualizer import Visualizer\n",
    "\n",
    "\n",
    "# 1.) Define Candidate class\n",
    "RowCandidate = candidate_subclass(\"RowCandidate\", [Data, Row])\n",
    "ColCandidate = candidate_subclass(\"ColCandidate\", [Data, Col])\n",
    "\n",
    "has_candidates = (\n",
    "    session.query(RowCandidate).filter(RowCandidate.split == 0).count() > 0 or\n",
    "    session.query(ColCandidate).filter(ColCandidate.split == 0).count() > 0\n",
    ")\n",
    "\n",
    "# 2.) DefineThrottlers\n",
    "def row_filter(c):\n",
    "    (data, row) = c\n",
    "     # Ignore only empty candidate values\n",
    "    if (re.match(\"^[\\., -]*$\", data.context.get_span())):\n",
    "        return False\n",
    "    if same_table((data, row)):\n",
    "        d = data.context.sentence\n",
    "        r = row.context.sentence\n",
    "        return (is_row_aligned(d, r)) # and is_horz_aligned((data, row)))\n",
    "    return True\n",
    "\n",
    "def col_filter(c):\n",
    "    (data, col) = c\n",
    "    # Ignore only empty candidate values\n",
    "    if (re.match(\"^[\\., -]*$\", data.context.get_span())):\n",
    "        return False\n",
    "    if same_table((data, col)):\n",
    "        d = data.context.sentence\n",
    "        c = col.context.sentence\n",
    "        return (is_col_aligned(d, c)) # and is_vert_aligned((data, col)))\n",
    "    return True\n",
    "\n",
    "\n",
    "# 3.) Candidate extraction\n",
    "candidate_extractor = CandidateExtractor(session, [RowCandidate], throttlers=[row_filter])\n",
    "\n",
    "for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
    "    if (not has_candidates):\n",
    "        candidate_extractor.apply(docs, split=i, parallelism=PARALLEL)\n",
    "    print(f\"Number of Candidates in split={i}: {session.query(RowCandidate).filter(RowCandidate.split == i).count()}\")\n",
    "\n",
    "cands_row = [\n",
    "    candidate_extractor.get_candidates(split = 0),\n",
    "    candidate_extractor.get_candidates(split = 1),\n",
    "    candidate_extractor.get_candidates(split = 2),\n",
    "]\n",
    "\n",
    "candidate_extractor = CandidateExtractor(session, [ColCandidate], throttlers=[col_filter])\n",
    "\n",
    "for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
    "    if (not has_candidates):\n",
    "        candidate_extractor.apply(docs, split=i, parallelism=PARALLEL)\n",
    "    print(f\"Number of Candidates in split={i}: {session.query(ColCandidate).filter(ColCandidate.split == i).count()}\")\n",
    "\n",
    "cands_col = [\n",
    "    candidate_extractor.get_candidates(split = 0),\n",
    "    candidate_extractor.get_candidates(split = 1),\n",
    "    candidate_extractor.get_candidates(split = 2),\n",
    "]\n",
    "                \n",
    "cands = [cands_row, cands_col]\n",
    "\n",
    "# 4.) Visualize some candidate for error analysis\n",
    "train_cand = cands[0][0][0][2]\n",
    "pprint(train_cand)\n",
    "vis = Visualizer(pdf_path)\n",
    "\n",
    "# Display a candidate\n",
    "vis.display_candidates([train_cand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Rule-based Pair-wise Evaluation Test\n",
    "\n",
    "We test the performance only based on the cell annotation rules for mentions (similar to TabbyXL rule-based algorithm), without any trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from troy200_utils import run_pairwise_eval\n",
    "\n",
    "%time row_results_train = run_pairwise_eval(gold_file, ATTRIBUTE, cands, all_docs, 'row', 0)\n",
    "%time col_results_train = run_pairwise_eval(gold_file, ATTRIBUTE, cands, all_docs, 'col', 0)\n",
    "\n",
    "%time row_results_dev = run_pairwise_eval(gold_file, ATTRIBUTE, cands, all_docs, 'row', 1)\n",
    "%time col_results_dev = run_pairwise_eval(gold_file, ATTRIBUTE, cands, all_docs, 'col', 1)\n",
    "\n",
    "%time row_results_test = run_pairwise_eval(gold_file, ATTRIBUTE, cands, all_docs, 'row', 2)\n",
    "%time col_results_test = run_pairwise_eval(gold_file, ATTRIBUTE, cands, all_docs, 'col', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results):\n",
    "    row_results_train = results[0][0]\n",
    "    row_results_dev = results[0][1]\n",
    "    row_results_test = results[0][2]\n",
    "    col_results_train = results[1][0]\n",
    "    col_results_dev = results[1][1]\n",
    "    col_results_test = results[1][2]\n",
    "    \n",
    "    prec_test = (\n",
    "        len(row_results_test[0]) + \n",
    "        len(col_results_test[0])\n",
    "    ) / (\n",
    "        len(row_results_test[0]) + \n",
    "        len(col_results_test[0]) +\n",
    "        len(row_results_test[1]) + \n",
    "        len(col_results_test[1])\n",
    "    )\n",
    "\n",
    "    rec_test = (\n",
    "        len(row_results_test[0]) + \n",
    "        len(col_results_test[0])\n",
    "    ) / (\n",
    "        len(row_results_test[0]) + \n",
    "        len(col_results_test[0]) +\n",
    "        len(row_results_test[2]) + \n",
    "        len(col_results_test[2])\n",
    "    )\n",
    "    f1_test = 2 * (prec_test * rec_test) / (prec_test + rec_test)\n",
    "\n",
    "    pos_total = (\n",
    "        len(row_results_train[0]) + \n",
    "        len(col_results_train[0]) + \n",
    "        len(row_results_dev[0]) + \n",
    "        len(col_results_dev[0]) + \n",
    "        len(row_results_test[0]) + \n",
    "        len(col_results_test[0])\n",
    "    )\n",
    "    prec_total = pos_total / (\n",
    "        pos_total + \n",
    "        len(row_results_train[1]) + \n",
    "        len(col_results_train[1]) + \n",
    "        len(row_results_dev[1]) + \n",
    "        len(col_results_dev[1]) + \n",
    "        len(row_results_test[1]) + \n",
    "        len(col_results_test[1])\n",
    "    )\n",
    "    rec_total = pos_total / (\n",
    "        pos_total + \n",
    "        len(row_results_train[2]) + \n",
    "        len(col_results_train[2]) + \n",
    "        len(row_results_dev[2]) + \n",
    "        len(col_results_dev[2]) + \n",
    "        len(row_results_test[2]) + \n",
    "        len(col_results_test[2])\n",
    "    )\n",
    "    f1_total = 2 * (prec_total * rec_total) / (prec_total + rec_total)\n",
    "    \n",
    "    return (prec_test, rec_test, f1_test, prec_total, rec_total, f1_total)\n",
    "\n",
    "\n",
    "(prec_test, rec_test, f1_test, prec_total, rec_total, f1_total) = summarize_results(\n",
    "[\n",
    "    [row_results_train, row_results_dev, row_results_test],\n",
    "    [col_results_train, col_results_dev, col_results_test]\n",
    "])\n",
    "print(f\"TOTAL DOCS PAIRWISE: Precision={prec_total}, Recall={rec_total}, F1={f1_total}\")\n",
    "print(f\"TEST PAIRWISE: Precision={prec_test}, Recall={rec_test}, F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Rule-based Candidate Evaluation Test\n",
    "\n",
    "We also test the performance on merged candidates, so correct row and column labels for a specific data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from troy200_utils import merge_candidates, entity_level_f1\n",
    "\n",
    "cands_merged = merge_candidates(cands[0][2][0], cands[1][2][0])\n",
    "%time (TP, FP, FN) = entity_level_f1(cands_merged, gold_file, ATTRIBUTE, test_docs, row_on=True, col_on=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further analyze the false-positive and false-negative results via a simple counter-interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from troy200_utils import Counter\n",
    "from ipywidgets import widgets\n",
    "from functools import partial\n",
    "from IPython.display import display\n",
    "\n",
    "# Buttons\n",
    "minus = widgets.Button(description='<')\n",
    "plus = widgets.Button(description='>')\n",
    "\n",
    "display(minus)\n",
    "display(plus)\n",
    "\n",
    "counter = Counter(\n",
    "    params=(\n",
    "        gold_file, \n",
    "        FP, \n",
    "        FN, \n",
    "        mentions,\n",
    "        [Data, Row, Col],\n",
    "        cands_merged,\n",
    "    ), \n",
    "    plus_btn=plus,\n",
    "    minus_btn=minus, \n",
    "    d_type=\"fn\", # \"fp\"\n",
    "    initial=0, \n",
    "    maximum=len(FN)-1, # len(FP)-1, \n",
    ")\n",
    "\n",
    "def btn_inc(counter, w):\n",
    "    counter.increment()  \n",
    "    counter.display()\n",
    "\n",
    "def btn_dec(counter, w):\n",
    "    counter.decrement()\n",
    "    counter.display()\n",
    "\n",
    "minus.on_click(partial(btn_dec, counter))\n",
    "plus.on_click(partial(btn_inc, counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 3: Supervised Classification\n",
    "\n",
    "1. Featurize the candidates\n",
    "2. Load Gold Data\n",
    "3. Build and train a descriminative model and test on the test set\n",
    "\n",
    "\n",
    "For this data set we do not use the labeling functions provided by Fonduer, as it is extremely small and we have gold labels for all instances.\n",
    "\n",
    "### 3.1) Featurize the candidates\n",
    "\n",
    "\"\"\"\n",
    "Unlike dealing with plain unstructured text, `Fonduer` deals with richly formatted data, and consequently featurizes each candidate with a baseline library of multimodal features. \n",
    "\n",
    "### Featurize with `Fonduer`'s optimized Postgres Featurizer\n",
    "We now annotate the candidates in our training, dev, and test sets with features. The `Featurizer` provided by `Fonduer` allows this to be done in parallel to improve performance.\n",
    "\n",
    "View the API provided by the `Featurizer` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/features.html#fonduer.features.Featurizer).\n",
    "\n",
    "At the end of this phase, `Fonduer` has generated the set of candidates and the feature matrix. Note that Phase 1 and 2 are relatively static and typically are only executed once during the KBC process.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.features import Featurizer\n",
    "from fonduer.features.models import Feature\n",
    "\n",
    "has_features = session.query(Feature).count() > 0\n",
    "\n",
    "# Features for row/column candidates (train, dev, test)\n",
    "F = []\n",
    "\n",
    "for i, cands_align in enumerate(cands):\n",
    "    featurizer = Featurizer(session, [RowCandidate]) if i == 0 else Featurizer(session, [ColCandidate])\n",
    "    train_cands = cands_align[0]\n",
    "    dev_cands = cands_align[1]\n",
    "    test_cands = cands_align[2]\n",
    "\n",
    "    if (not has_features):\n",
    "        # Training set\n",
    "        %time featurizer.apply(split=0, train=True, parallelism=PARALLEL)\n",
    "        %time F_train = featurizer.get_feature_matrices(train_cands)\n",
    "        print(F_train[0].shape)\n",
    "\n",
    "        # Dev set\n",
    "        %time featurizer.apply(split=1, parallelism=PARALLEL)\n",
    "        %time F_dev = featurizer.get_feature_matrices(dev_cands)\n",
    "        print(F_dev[0].shape)\n",
    "\n",
    "        # Test set\n",
    "        %time featurizer.apply(split=2, parallelism=PARALLEL)\n",
    "        %time F_test = featurizer.get_feature_matrices(test_cands)\n",
    "        print(F_test[0].shape)\n",
    "    else:\n",
    "        %time F_train = featurizer.get_feature_matrices(train_cands)\n",
    "        %time F_dev = featurizer.get_feature_matrices(dev_cands)\n",
    "        %time F_test = featurizer.get_feature_matrices(test_cands)\n",
    "    # Summarize for row/col\n",
    "    F.append([F_train, F_dev, F_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Loading Gold LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fonduer.supervision.models import GoldLabel\n",
    "from fonduer.supervision import Labeler\n",
    "from troy200_utils import get_gold_func\n",
    "\n",
    "# 1.1) Load the gold data rows\n",
    "gold_row = get_gold_func(gold_file, row_on=True, col_on=False)\n",
    "docs = corpus_parser.get_documents()\n",
    "labeler = Labeler(session, [RowCandidate])\n",
    "%time labeler.apply(docs=docs, lfs=[[gold_row]], table=GoldLabel, train=True, parallelism=PARALLEL)\n",
    "\n",
    "# 1.2) Load the gold data cols\n",
    "gold_col = get_gold_func(gold_file, row_on=False, col_on=True)\n",
    "docs = corpus_parser.get_documents()\n",
    "labeler = Labeler(session, [ColCandidate])\n",
    "%time labeler.apply(docs=docs, lfs=[[gold_col]], table=GoldLabel, train=True, parallelism=PARALLEL)\n",
    "\n",
    "gold = [gold_row, gold_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Training the Discriminative Model \n",
    "\n",
    "Fonduer uses the machine learning framework [Emmental](https://github.com/SenWu/emmental) to support all model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emmental\n",
    "import numpy as np\n",
    "\n",
    "from emmental.modules.embedding_module import EmbeddingModule\n",
    "from emmental.data import EmmentalDataLoader\n",
    "from emmental.model import EmmentalModel\n",
    "from emmental.learner import EmmentalLearner\n",
    "from fonduer.learning.utils import collect_word_counter\n",
    "from fonduer.learning.dataset import FonduerDataset\n",
    "from fonduer.learning.task import create_task\n",
    "\n",
    "ABSTAIN = -1\n",
    "FALSE = 0\n",
    "TRUE = 1\n",
    "\n",
    "def train_model(cands, F, align_type, model_type=\"LogisticRegression\"):\n",
    "    # Extract candidates and features based on the align type (row/column)\n",
    "    align_val = 0 if align_type == \"row\" else 1\n",
    "    train_cands = cands[align_val][0]\n",
    "    F_train = F[align_val][0]\n",
    "    train_marginals = np.array([[0,1] if gold[align_val](x) else [1,0] for x in train_cands[0]])\n",
    "    \n",
    "    # 1.) Setup training config\n",
    "    config = {\n",
    "        \"meta_config\": {\"verbose\": True},\n",
    "        \"model_config\": {\"model_path\": None, \"device\": 0, \"dataparallel\": False},\n",
    "        \"learner_config\": {\n",
    "            \"n_epochs\": 50,\n",
    "            \"optimizer_config\": {\"lr\": 0.001, \"l2\": 0.0},\n",
    "            \"task_scheduler\": \"round_robin\",\n",
    "        },\n",
    "        \"logging_config\": {\n",
    "            \"evaluation_freq\": 1,\n",
    "            \"counter_unit\": \"epoch\",\n",
    "            \"checkpointing\": False,\n",
    "            \"checkpointer_config\": {\n",
    "                \"checkpoint_metric\": {f\"{ATTRIBUTE}/{ATTRIBUTE}/train/loss\": \"min\"},\n",
    "                \"checkpoint_freq\": 1,\n",
    "                \"checkpoint_runway\": 2,\n",
    "                \"clear_intermediate_checkpoints\": True,\n",
    "                \"clear_all_checkpoints\": True,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    emmental.init(Meta.log_path)\n",
    "    emmental.Meta.update_config(config=config)\n",
    "    \n",
    "    # 2.) Collect word counter from training data\n",
    "    word_counter = collect_word_counter(train_cands)\n",
    "    \n",
    "    # 3.) Generate word embedding module for LSTM model\n",
    "    # (in Logistic Regression, we generate it since Fonduer dataset requires word2id dict)\n",
    "    # Geneate special tokens\n",
    "    arity = 2\n",
    "    specials = []\n",
    "    for i in range(arity):\n",
    "        specials += [f\"~~[[{i}\", f\"{i}]]~~\"]\n",
    "\n",
    "    emb_layer = EmbeddingModule(\n",
    "        word_counter=word_counter, word_dim=300, specials=specials\n",
    "    )\n",
    "    \n",
    "    # 4.) Generate dataloader for training set\n",
    "    # No noise in Gold labels\n",
    "    train_dataloader = EmmentalDataLoader(\n",
    "        task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "        dataset=FonduerDataset(\n",
    "            ATTRIBUTE,\n",
    "            train_cands[0],\n",
    "            F_train[0],\n",
    "            emb_layer.word2id,\n",
    "            train_marginals,\n",
    "        ),\n",
    "        split=\"train\",\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    # 5.) Training \n",
    "    tasks = create_task(\n",
    "        ATTRIBUTE, 2, F_train[0].shape[1], 2, emb_layer, model=model_type # \"LSTM\" \n",
    "    )\n",
    "\n",
    "    model = EmmentalModel(name=f\"{ATTRIBUTE}_task\")\n",
    "\n",
    "    for task in tasks:\n",
    "        model.add_task(task)\n",
    "\n",
    "    emmental_learner = EmmentalLearner()\n",
    "    emmental_learner.learn(model, [train_dataloader])\n",
    "    \n",
    "    return (model, emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, emb_layer, cands, F, align_type = \"row\"):\n",
    "    # Extract candidates and features based on the align type (row/column)\n",
    "    align_val = 0 if align_type == \"row\" else 1\n",
    "    train_cands = cands[align_val][0]\n",
    "    dev_cands = cands[align_val][1]\n",
    "    test_cands = cands[align_val][2] \n",
    "    F_train = F[align_val][0]\n",
    "    F_dev = F[align_val][1]\n",
    "    F_test = F[align_val][2]\n",
    "    row_on = True if align_type == \"row\" else False\n",
    "    col_on = True if align_type == \"col\" else False\n",
    "    \n",
    "    # Generate dataloader for test data\n",
    "    test_dataloader = EmmentalDataLoader(\n",
    "        task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "        dataset=FonduerDataset(\n",
    "            ATTRIBUTE, test_cands[0], F_test[0], emb_layer.word2id, 2\n",
    "        ),\n",
    "        split=\"test\",\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    test_preds = model.predict(test_dataloader, return_preds=True)\n",
    "    positive = np.where(np.array(test_preds[\"probs\"][ATTRIBUTE])[:, TRUE] > 0.6)\n",
    "    true_pred = [test_cands[0][_] for _ in positive[0]]\n",
    "    test_results = entity_level_f1(true_pred, gold_file, ATTRIBUTE, test_docs, row_on=row_on, col_on=col_on)\n",
    "    \n",
    "    # Run on dev and train set for validation\n",
    "    # We run the predictions also on our training and dev set, to validate that everything seems to work smoothly\n",
    "    \n",
    "    # Generate dataloader for dev data\n",
    "    dev_dataloader = EmmentalDataLoader(\n",
    "        task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "        dataset=FonduerDataset(\n",
    "            ATTRIBUTE, dev_cands[0], F_dev[0], emb_layer.word2id, 2\n",
    "        ),\n",
    "        split=\"test\",\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    dev_preds = model.predict(dev_dataloader, return_preds=True)\n",
    "    positive_dev = np.where(np.array(dev_preds[\"probs\"][ATTRIBUTE])[:, TRUE] > 0.6)\n",
    "    true_dev_pred = [dev_cands[0][_] for _ in positive_dev[0]]\n",
    "    dev_results = entity_level_f1(true_dev_pred, gold_file, ATTRIBUTE, dev_docs, row_on=row_on, col_on=col_on)\n",
    "    \n",
    "    # Generate dataloader for train data\n",
    "    train_dataloader = EmmentalDataLoader(\n",
    "        task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "        dataset=FonduerDataset(\n",
    "            ATTRIBUTE, train_cands[0], F_train[0], emb_layer.word2id, 2\n",
    "        ),\n",
    "        split=\"test\",\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    train_preds = model.predict(train_dataloader, return_preds=True)\n",
    "    positive_train = np.where(np.array(train_preds[\"probs\"][ATTRIBUTE])[:, TRUE] > 0.6)\n",
    "    true_train_pred = [train_cands[0][_] for _ in positive_train[0]]\n",
    "    train_results = entity_level_f1(true_train_pred, gold_file, ATTRIBUTE, train_docs, row_on=row_on, col_on=col_on)\n",
    "        \n",
    "    return [train_results, dev_results, test_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on the Test Set \n",
    "\n",
    "We keep the results from the rule-based approaches in mind.\n",
    "\n",
    "```\n",
    "========================================\n",
    "Scoring on Entity-Level Gold Data for only ROW and TEST\n",
    "========================================\n",
    "Corpus Precision 1.0\n",
    "Corpus Recall    0.993\n",
    "Corpus F1        0.996\n",
    "----------------------------------------\n",
    "TP: 2301 | FP: 0 | FN: 17\n",
    "========================================\n",
    "\n",
    "\n",
    "========================================\n",
    "Scoring on Entity-Level Gold Data only COL and TEST\n",
    "========================================\n",
    "Corpus Precision 0.98\n",
    "Corpus Recall    0.973\n",
    "Corpus F1        0.977\n",
    "----------------------------------------\n",
    "TP: 3149 | FP: 64 | FN: 87\n",
    "========================================\n",
    "```\n",
    "\n",
    "\n",
    "* TOTAL DOCS PAIRWISE: Precision=0.9906678865507776, Recall=0.9821951535570818, F1=0.9864133263925039\n",
    "* TEST PAIRWISE: Precision=0.9883931809938339, Recall=0.981274756931941, F1=0.9848211058908565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and evaluate for rows\n",
    "(row_model, row_emb_layer) = train_model(cands, F, \"row\")\n",
    "row_results = eval_model(row_model, row_emb_layer, cands, F, \"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and evaluate for columns\n",
    "(col_model, col_emb_layer) = train_model(cands, F, \"col\")\n",
    "col_results = eval_model(col_model, col_emb_layer, cands, F, \"col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prec_test, rec_test, f1_test, prec_total, rec_total, f1_total) = summarize_results([row_results, col_results])\n",
    "print(f\"TOTAL DOCS PAIRWISE (LR): Precision={prec_total}, Recall={rec_total}, F1={f1_total}\")\n",
    "print(f\"TEST PAIRWISE (LR): Precision={prec_test}, Recall={rec_test}, F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a much lower recall for the training set.\n",
    "This could be due to the 1-5 documents that have formatting issues in the gold standard and thus yield all candidates incorrect. E.g.\n",
    "\n",
    "* C10067 (x not included and footnotes)\n",
    "* C10086 (Date formatting)\n",
    "* C10106 (Wrong goldset column headers in gold)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to LSTM training\n",
    "\n",
    "This will take much longer. Lets see how the Bi-LSTM performs, even though the number of training samples is very small (~15k-80k candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and evaluate for rows\n",
    "(row_model, row_emb_layer) = train_model(cands, F, \"row\", \"LSTM\" )\n",
    "row_results = eval_model(row_model, row_emb_layer, cands, F, \"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and evaluate for columns\n",
    "(col_model, col_emb_layer) = train_model(cands, F, \"col\", \"LSTM\" )\n",
    "col_results = eval_model(col_model, col_emb_layer, cands, F, \"col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prec_test, rec_test, f1_test, prec_total, rec_total, f1_total) = summarize_results([row_results, col_results])\n",
    "print(f\"TOTAL DOCS PAIRWISE (LSTM): Precision={prec_total}, Recall={rec_total}, F1={f1_total}\")\n",
    "print(f\"TEST PAIRWISE (LSTM): Precision={prec_test}, Recall={rec_test}, F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Error Analysis & Iterative KBC \n",
    "\n",
    "- Analyise the false positive (FP) and false negative (FN) candidates\n",
    "- Use the visualization tool to better understand the errors\n",
    "\n",
    "We could theoretically improve on this data set by iterating over more pre-processing assumptions (e.g. formatting dates, x/X discard/keep, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
